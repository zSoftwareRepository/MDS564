{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9da584fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:90% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>div.output_area{max-height:10000px;overflow:scroll;}</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python: 3.10.9 | packaged by conda-forge | (main, Jan 11 2023, 15:15:40) [MSC v.1916 64 bit (AMD64)]\n",
      "Notebook Last Run Initiated: 2023-03-13 19:40:21.810241\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload\n",
    "\n",
    "## This expands a notebook to full width\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "display(HTML(\"<style>.container { width:90% !important; }</style>\"))\n",
    "display(HTML(\"\"\"<style>div.output_area{max-height:10000px;overflow:scroll;}</style>\"\"\"))\n",
    "## Show Python Version\n",
    "import sys\n",
    "print(\"Python: {0}\".format(sys.version))\n",
    "\n",
    "## Show Current Time\n",
    "import datetime as dt\n",
    "start = dt.datetime.now()\n",
    "print(\"Notebook Last Run Initiated: \"+str(start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f7da849f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import tensorflow as tf\n",
    "\n",
    "import random\n",
    "\n",
    "from tensorflow.keras.layers import TextVectorization\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.utils import Sequence\n",
    "\n",
    "from tensorflow import keras\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "47f01809",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQUENCE_LENGTH = 14\n",
    "\n",
    "reserved_tokens = [\"[PAD]\", \"[UNK]\", \"[START]\", \"[END]\"]\n",
    "\n",
    "seed = 23\n",
    "\n",
    "learning_rate = 0.001\n",
    "weight_decay = 0.0001\n",
    "batch_size = 128\n",
    "num_epochs = 500\n",
    "projection_dim = 300\n",
    "num_heads = 10\n",
    "\n",
    "transformer_units = [\n",
    "    projection_dim * 2,\n",
    "    projection_dim,\n",
    "]  # Size of the transformer layers\n",
    "\n",
    "transformer_layers = 18\n",
    "\n",
    "embed_dim = projection_dim \n",
    "embeddings_shape = (1,embed_dim)\n",
    "\n",
    "mlp_head_units = [2048, 1024]  # Size of the dense layers of the final classifier\n",
    "\n",
    "ckp_path = 'models/Model_Embedding_transformers_v3.hdf5'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a6f275f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.utils.set_random_seed(seed)\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c5186708",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGenerator(Sequence):\n",
    "        \n",
    "    def __init__(self, df_X, batch_size=32, shuffle=False):\n",
    "        self.batch_size = batch_size\n",
    "        self.df_X = df_X\n",
    "        self.indices = self.df_X.index.tolist()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.shuffle = shuffle\n",
    "        self.on_epoch_end()\n",
    "        \n",
    "    def __len__(self):\n",
    "        return int(np.floor(len(self.indices) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        index = self.index[index * self.batch_size:(index + 1) * self.batch_size]\n",
    "        batch = [self.indices[k] for k in index]\n",
    "        \n",
    "        X, y = self.__get_data(batch)\n",
    "        return X, y\n",
    "    \n",
    "    def n(self):\n",
    "        return len(self.indices)\n",
    "    \n",
    "    def on_epoch_end(self):\n",
    "        self.index = np.arange(len(self.indices))\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.index)\n",
    "\n",
    "    def __get_data(self, batch):\n",
    "        X1 = []\n",
    "        y1 = []\n",
    "        \n",
    "        for i, id in enumerate(batch):\n",
    "            \n",
    "            column = random.randrange(MAX_SEQUENCE_LENGTH)\n",
    "            \n",
    "            # Data\n",
    "            texts = self.df_X.iloc[self.indices[id]]\n",
    "            \n",
    "            #Labels\n",
    "            label = texts[column]\n",
    "            texts[column] = 0\n",
    "\n",
    "            X1.append(texts)\n",
    "            y1.append(label)\n",
    "\n",
    "                            \n",
    "        return np.array(X1), np.array(y1).reshape(self.batch_size,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4c5671c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input vector, returns nearest word(s)\n",
    "def Cosine_Similarity(word,weight,word_to_index,vocab_size,index_to_word):\n",
    "    \n",
    "    #Get the index of the word from the dictionary\n",
    "    index = word_to_index[word]\n",
    "    \n",
    "    #Get the correspondin weights for the word\n",
    "    word_vector_1 = weight[index]\n",
    "    \n",
    "    word_similarity = {}\n",
    "\n",
    "    for i in range(vocab_size):\n",
    "        \n",
    "        j = i\n",
    "        \n",
    "        word_vector_2 = weight[j]\n",
    "        \n",
    "        theta_sum = np.dot(word_vector_1, word_vector_2)\n",
    "        theta_den = np.linalg.norm(word_vector_1) * np.linalg.norm(word_vector_2)\n",
    "        theta = theta_sum / theta_den\n",
    "        \n",
    "        word = index_to_word[j]\n",
    "        word_similarity[word] = theta\n",
    "    \n",
    "    return word_similarity #words_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "790f21fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionEmbedding(layers.Layer):\n",
    "    def __init__(self, maxlen, embed_dim):\n",
    "        super().__init__()\n",
    "        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n",
    "\n",
    "    def call(self, x):\n",
    "        maxlen = tf.shape(x)[1]\n",
    "        positions = tf.range(start=0, limit=maxlen, delta=1)\n",
    "        positions = self.pos_emb(positions)\n",
    "        return positions + x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "75a06fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionEmbeddingFixedWeights(layers.Layer):\n",
    "    def __init__(self, sequence_length, output_dim, **kwargs):\n",
    "        super(PositionEmbeddingFixedWeights, self).__init__(**kwargs)\n",
    "        \n",
    "        position_embedding_matrix = self.get_position_encoding(sequence_length, output_dim) \n",
    "        \n",
    "        self.position_embedding_layer = layers.Embedding(\n",
    "            input_dim=sequence_length, output_dim=output_dim,\n",
    "            weights=[position_embedding_matrix],\n",
    "            trainable=False\n",
    "        )\n",
    "             \n",
    "    def get_position_encoding(self, seq_len, d, n=10000):\n",
    "        P = np.zeros((seq_len, d))\n",
    "        for k in range(seq_len):\n",
    "            for i in np.arange(int(d/2)):\n",
    "                denominator = np.power(n, 2*i/d)\n",
    "                P[k, 2*i] = np.sin(k/denominator)\n",
    "                P[k, 2*i+1] = np.cos(k/denominator)\n",
    "        return P\n",
    "\n",
    "\n",
    "    def call(self, inputs):        \n",
    "        position_indices = tf.range(tf.shape(inputs)[1])\n",
    "        embedded_indices = self.position_embedding_layer(position_indices)\n",
    "        return inputs + embedded_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "14982440",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlp(x, hidden_units, dropout_rate):\n",
    "    for units in hidden_units:\n",
    "        x = layers.Dense(units, activation=tf.nn.gelu)(x)\n",
    "        x = layers.Dropout(dropout_rate)(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f03b113b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_classifier(learning_rate):\n",
    "    \n",
    "    input_text  = layers.Input(shape=(MAX_SEQUENCE_LENGTH-1,),dtype=\"int32\",name='input_text')\n",
    "    \n",
    "    embeddings = tf.keras.layers.Embedding(input_dim=vocab_size, input_length=MAX_SEQUENCE_LENGTH, output_dim=embed_dim,name='embeddings')(input_text)\n",
    "    \n",
    "    embeddings = PositionEmbeddingFixedWeights(sequence_length=MAX_SEQUENCE_LENGTH-1,output_dim=embed_dim)(embeddings)\n",
    "    \n",
    "    # Create multiple layers of the Transformer block.\n",
    "    for _ in range(transformer_layers):\n",
    "        # Layer normalization 1.\n",
    "        x1 = layers.LayerNormalization(epsilon=1e-6)(embeddings)\n",
    "        # Create a multi-head attention layer.\n",
    "        attention_output = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=projection_dim, dropout=0.1)(x1, x1)\n",
    "        # Skip connection 1.\n",
    "        x2 = layers.Add()([attention_output, embeddings])\n",
    "        # Layer normalization 2.\n",
    "        x3 = layers.LayerNormalization(epsilon=1e-6)(x2)\n",
    "        # MLP.\n",
    "        x3 = mlp(x3, hidden_units=transformer_units, dropout_rate=0.1)\n",
    "        # Skip connection 2.\n",
    "        embeddings = layers.Add()([x3, x2])\n",
    "\n",
    "    # Create a [batch_size, projection_dim] tensor.\n",
    "    representation = layers.LayerNormalization(epsilon=1e-6)(embeddings)\n",
    "    representation = layers.GlobalAveragePooling1D()(representation)\n",
    "    representation = layers.Dropout(0.1)(representation)\n",
    "    \n",
    "    # Add MLP.\n",
    "    features = mlp(representation, hidden_units=mlp_head_units, dropout_rate=0.1)\n",
    "    \n",
    "    # Classify outputs.\n",
    "    outputs = layers.Dense(num_classes,activation='softmax', name='activation')(features)\n",
    "    \n",
    "    # Create the Keras model.\n",
    "    model = Model(inputs=input_text, outputs=outputs,name='Customer_Code_Embeddings')\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate = learning_rate),loss=\"sparse_categorical_crossentropy\")\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f7c9e9ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input vector, returns nearest word(s)\n",
    "def Cosine_Similarity(word,weight,word_to_index,vocab_size,index_to_word):\n",
    "    \n",
    "    #Get the index of the word from the dictionary\n",
    "    index = word_to_index[word]\n",
    "    \n",
    "    #Get the correspondin weights for the word\n",
    "    word_vector_1 = weight[index]\n",
    "    \n",
    "    word_similarity = {}\n",
    "\n",
    "    for i in range(vocab_size):\n",
    "        \n",
    "        j = i\n",
    "        \n",
    "        word_vector_2 = weight[j]\n",
    "        \n",
    "        theta_sum = np.dot(word_vector_1, word_vector_2)\n",
    "        theta_den = np.linalg.norm(word_vector_1) * np.linalg.norm(word_vector_2)\n",
    "        theta = theta_sum / theta_den\n",
    "        \n",
    "        word = index_to_word[j]\n",
    "        word_similarity[word] = theta\n",
    "    \n",
    "    return word_similarity #words_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "16648b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data/train_data_cat.csv')\n",
    "data.postal_code = data.postal_code.astype(str)\n",
    "data['postal_code'] = data['postal_code'].str.zfill(5)\n",
    "data = data.drop(['accountid','userid'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "428af28d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['data'] = data.apply(' '.join, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "85552dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = pd.DataFrame()\n",
    "texts['data'] = data['data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b3a8f0eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_lines = []\n",
    "for index, row in texts.iterrows():\n",
    "    text_lines.append(row['data'].split(\" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ece082da",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_tokens = [item for text_lines in text_lines for item in text_lines]\n",
    "texts = list(np.unique(np.array(text_tokens)))\n",
    "VOCAB_SIZE = len(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0c2e1c70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12171\n"
     ]
    }
   ],
   "source": [
    "print(VOCAB_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4faf6ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the tokenizer\n",
    "tokenizer = Tokenizer(filters=' ')\n",
    "# fit the tokenizer on the documents\n",
    "tokenizer.fit_on_texts(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "675643e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_docs = tokenizer.texts_to_sequences(text_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ac6d22d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# integer encode the documents\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "num_classes = vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b06ca524",
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_docs = pad_sequences(encoded_docs, maxlen=MAX_SEQUENCE_LENGTH, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "62fb38b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.DataFrame(padded_docs)\n",
    "train_generator = DataGenerator(df_X=train, batch_size=batch_size, shuffle=True)\n",
    "STEP_SIZE_TRAIN=train_generator.n()//train_generator.batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "16c61213",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "153/153 [==============================] - 67s 336ms/step - loss: 5.3768 - lr: 0.0010\n",
      "Epoch 2/500\n",
      "153/153 [==============================] - 51s 333ms/step - loss: 5.4299 - lr: 0.0010\n",
      "Epoch 3/500\n",
      "153/153 [==============================] - 52s 339ms/step - loss: 4.7672 - lr: 0.0010\n",
      "Epoch 4/500\n",
      "153/153 [==============================] - 52s 339ms/step - loss: 4.5358 - lr: 0.0010\n",
      "Epoch 5/500\n",
      "153/153 [==============================] - 52s 339ms/step - loss: 4.3632 - lr: 0.0010\n",
      "Epoch 6/500\n",
      "153/153 [==============================] - 52s 340ms/step - loss: 4.1611 - lr: 0.0010\n",
      "Epoch 7/500\n",
      "153/153 [==============================] - 52s 340ms/step - loss: 3.9275 - lr: 0.0010\n",
      "Epoch 8/500\n",
      "153/153 [==============================] - 52s 340ms/step - loss: 3.7312 - lr: 0.0010\n",
      "Epoch 9/500\n",
      "153/153 [==============================] - 52s 340ms/step - loss: 3.5353 - lr: 0.0010\n",
      "Epoch 10/500\n",
      "153/153 [==============================] - 52s 341ms/step - loss: 3.3214 - lr: 0.0010\n",
      "Epoch 11/500\n",
      "153/153 [==============================] - 52s 341ms/step - loss: 3.1112 - lr: 0.0010\n",
      "Epoch 12/500\n",
      "153/153 [==============================] - 52s 342ms/step - loss: 2.9496 - lr: 0.0010\n",
      "Epoch 13/500\n",
      "153/153 [==============================] - 52s 342ms/step - loss: 2.7755 - lr: 0.0010\n",
      "Epoch 14/500\n",
      "153/153 [==============================] - 52s 341ms/step - loss: 2.5955 - lr: 0.0010\n",
      "Epoch 15/500\n",
      "153/153 [==============================] - 52s 341ms/step - loss: 2.4634 - lr: 0.0010\n",
      "Epoch 16/500\n",
      "153/153 [==============================] - 52s 341ms/step - loss: 2.3181 - lr: 0.0010\n",
      "Epoch 17/500\n",
      "153/153 [==============================] - 52s 341ms/step - loss: 2.1868 - lr: 0.0010\n",
      "Epoch 18/500\n",
      "153/153 [==============================] - 52s 343ms/step - loss: 2.0775 - lr: 0.0010\n",
      "Epoch 19/500\n",
      "153/153 [==============================] - 52s 341ms/step - loss: 1.9617 - lr: 0.0010\n",
      "Epoch 20/500\n",
      "153/153 [==============================] - 52s 343ms/step - loss: 1.8192 - lr: 0.0010\n",
      "Epoch 21/500\n",
      "153/153 [==============================] - 52s 342ms/step - loss: 1.7290 - lr: 0.0010\n",
      "Epoch 22/500\n",
      "153/153 [==============================] - 52s 341ms/step - loss: 1.6276 - lr: 0.0010\n",
      "Epoch 23/500\n",
      "153/153 [==============================] - 52s 342ms/step - loss: 1.4980 - lr: 0.0010\n",
      "Epoch 24/500\n",
      "153/153 [==============================] - 52s 341ms/step - loss: 1.4262 - lr: 0.0010\n",
      "Epoch 25/500\n",
      "153/153 [==============================] - 52s 342ms/step - loss: 1.3153 - lr: 0.0010\n",
      "Epoch 26/500\n",
      "153/153 [==============================] - 52s 342ms/step - loss: 1.2662 - lr: 0.0010\n",
      "Epoch 27/500\n",
      "153/153 [==============================] - 52s 342ms/step - loss: 1.1748 - lr: 0.0010\n",
      "Epoch 28/500\n",
      "153/153 [==============================] - 52s 341ms/step - loss: 1.1210 - lr: 0.0010\n",
      "Epoch 29/500\n",
      "153/153 [==============================] - 52s 341ms/step - loss: 1.0205 - lr: 0.0010\n",
      "Epoch 30/500\n",
      "153/153 [==============================] - 52s 341ms/step - loss: 0.9620 - lr: 0.0010\n",
      "Epoch 31/500\n",
      "153/153 [==============================] - 52s 341ms/step - loss: 0.9223 - lr: 0.0010\n",
      "Epoch 32/500\n",
      "153/153 [==============================] - 52s 340ms/step - loss: 0.8236 - lr: 0.0010\n",
      "Epoch 33/500\n",
      "153/153 [==============================] - 51s 336ms/step - loss: 0.8330 - lr: 0.0010\n",
      "Epoch 34/500\n",
      "153/153 [==============================] - 52s 341ms/step - loss: 0.7589 - lr: 0.0010\n",
      "Epoch 35/500\n",
      "153/153 [==============================] - 52s 341ms/step - loss: 0.6965 - lr: 0.0010\n",
      "Epoch 36/500\n",
      "153/153 [==============================] - 52s 342ms/step - loss: 0.6802 - lr: 0.0010\n",
      "Epoch 37/500\n",
      "153/153 [==============================] - 53s 343ms/step - loss: 0.6280 - lr: 0.0010\n",
      "Epoch 38/500\n",
      "153/153 [==============================] - 52s 343ms/step - loss: 0.5681 - lr: 0.0010\n",
      "Epoch 39/500\n",
      "153/153 [==============================] - 52s 342ms/step - loss: 0.5427 - lr: 0.0010\n",
      "Epoch 40/500\n",
      "153/153 [==============================] - 52s 342ms/step - loss: 0.5153 - lr: 0.0010\n",
      "Epoch 41/500\n",
      "153/153 [==============================] - 52s 342ms/step - loss: 0.4677 - lr: 0.0010\n",
      "Epoch 42/500\n",
      "153/153 [==============================] - 52s 342ms/step - loss: 0.4530 - lr: 0.0010\n",
      "Epoch 43/500\n",
      "153/153 [==============================] - 52s 342ms/step - loss: 0.4002 - lr: 0.0010\n",
      "Epoch 44/500\n",
      "153/153 [==============================] - 52s 342ms/step - loss: 0.3689 - lr: 0.0010\n",
      "Epoch 45/500\n",
      "153/153 [==============================] - 53s 343ms/step - loss: 0.3525 - lr: 0.0010\n",
      "Epoch 46/500\n",
      "153/153 [==============================] - 52s 343ms/step - loss: 0.3356 - lr: 0.0010\n",
      "Epoch 47/500\n",
      "153/153 [==============================] - 52s 337ms/step - loss: 0.3510 - lr: 0.0010\n",
      "Epoch 48/500\n",
      "153/153 [==============================] - 52s 342ms/step - loss: 0.3016 - lr: 0.0010\n",
      "Epoch 49/500\n",
      "153/153 [==============================] - 52s 342ms/step - loss: 0.2733 - lr: 0.0010\n",
      "Epoch 50/500\n",
      "153/153 [==============================] - 53s 344ms/step - loss: 0.2521 - lr: 0.0010\n",
      "Epoch 51/500\n",
      "153/153 [==============================] - 53s 343ms/step - loss: 0.2432 - lr: 0.0010\n",
      "Epoch 52/500\n",
      "153/153 [==============================] - 52s 343ms/step - loss: 0.2250 - lr: 0.0010\n",
      "Epoch 53/500\n",
      "153/153 [==============================] - 52s 342ms/step - loss: 0.2040 - lr: 0.0010\n",
      "Epoch 54/500\n",
      "153/153 [==============================] - 53s 343ms/step - loss: 0.1933 - lr: 0.0010\n",
      "Epoch 55/500\n",
      "153/153 [==============================] - 52s 337ms/step - loss: 0.1959 - lr: 0.0010\n",
      "Epoch 56/500\n",
      "153/153 [==============================] - 52s 337ms/step - loss: 0.1964 - lr: 0.0010\n",
      "Epoch 57/500\n",
      "153/153 [==============================] - 52s 343ms/step - loss: 0.1628 - lr: 0.0010\n",
      "Epoch 58/500\n",
      "153/153 [==============================] - 52s 342ms/step - loss: 0.1455 - lr: 0.0010\n",
      "Epoch 59/500\n",
      "153/153 [==============================] - 52s 342ms/step - loss: 0.1276 - lr: 0.0010\n",
      "Epoch 60/500\n",
      "153/153 [==============================] - 52s 337ms/step - loss: 0.1300 - lr: 0.0010\n",
      "Epoch 61/500\n",
      "153/153 [==============================] - 52s 342ms/step - loss: 0.1218 - lr: 0.0010\n",
      "Epoch 62/500\n",
      "153/153 [==============================] - 52s 342ms/step - loss: 0.1138 - lr: 0.0010\n",
      "Epoch 63/500\n",
      "153/153 [==============================] - 53s 344ms/step - loss: 0.1115 - lr: 0.0010\n",
      "Epoch 64/500\n",
      "153/153 [==============================] - 52s 337ms/step - loss: 0.1218 - lr: 0.0010\n",
      "Epoch 65/500\n",
      "153/153 [==============================] - 52s 343ms/step - loss: 0.0832 - lr: 0.0010\n",
      "Epoch 66/500\n",
      "153/153 [==============================] - 52s 337ms/step - loss: 0.1049 - lr: 0.0010\n",
      "Epoch 67/500\n",
      "153/153 [==============================] - 52s 342ms/step - loss: 0.0821 - lr: 0.0010\n",
      "Epoch 68/500\n",
      "153/153 [==============================] - 52s 342ms/step - loss: 0.0730 - lr: 0.0010\n",
      "Epoch 69/500\n",
      "153/153 [==============================] - 52s 343ms/step - loss: 0.0663 - lr: 0.0010\n",
      "Epoch 70/500\n",
      "153/153 [==============================] - 52s 337ms/step - loss: 0.0740 - lr: 0.0010\n",
      "Epoch 71/500\n",
      "153/153 [==============================] - 51s 336ms/step - loss: 0.0680 - lr: 0.0010\n",
      "Epoch 72/500\n",
      "153/153 [==============================] - 52s 343ms/step - loss: 0.0614 - lr: 0.0010\n",
      "Epoch 73/500\n",
      "153/153 [==============================] - 52s 342ms/step - loss: 0.0578 - lr: 0.0010\n",
      "Epoch 74/500\n",
      "153/153 [==============================] - 52s 342ms/step - loss: 0.0553 - lr: 0.0010\n",
      "Epoch 75/500\n",
      "153/153 [==============================] - 52s 341ms/step - loss: 0.0533 - lr: 0.0010\n",
      "Epoch 76/500\n",
      "153/153 [==============================] - 52s 342ms/step - loss: 0.0488 - lr: 0.0010\n",
      "Epoch 77/500\n",
      "153/153 [==============================] - 52s 342ms/step - loss: 0.0427 - lr: 0.0010\n",
      "Epoch 78/500\n",
      "153/153 [==============================] - 52s 341ms/step - loss: 0.0372 - lr: 0.0010\n",
      "Epoch 79/500\n",
      "153/153 [==============================] - 52s 340ms/step - loss: 0.0308 - lr: 0.0010\n",
      "Epoch 80/500\n",
      "153/153 [==============================] - 51s 335ms/step - loss: 0.0379 - lr: 0.0010\n",
      "Epoch 81/500\n",
      "153/153 [==============================] - 51s 335ms/step - loss: 0.0367 - lr: 0.0010\n",
      "Epoch 82/500\n",
      "153/153 [==============================] - 52s 343ms/step - loss: 0.0294 - lr: 0.0010\n",
      "Epoch 83/500\n",
      "153/153 [==============================] - 51s 336ms/step - loss: 0.0378 - lr: 0.0010\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 84/500\n",
      "153/153 [==============================] - 52s 341ms/step - loss: 0.0255 - lr: 0.0010\n",
      "Epoch 85/500\n",
      "153/153 [==============================] - 52s 341ms/step - loss: 0.0222 - lr: 0.0010\n",
      "Epoch 86/500\n",
      "153/153 [==============================] - 51s 336ms/step - loss: 0.0238 - lr: 0.0010\n",
      "Epoch 87/500\n",
      "153/153 [==============================] - 51s 335ms/step - loss: 0.0255 - lr: 0.0010\n",
      "Epoch 88/500\n",
      "153/153 [==============================] - 52s 342ms/step - loss: 0.0197 - lr: 0.0010\n",
      "Epoch 89/500\n",
      "153/153 [==============================] - 53s 343ms/step - loss: 0.0159 - lr: 0.0010\n",
      "Epoch 90/500\n",
      "153/153 [==============================] - 51s 337ms/step - loss: 0.0163 - lr: 0.0010\n",
      "Epoch 91/500\n",
      "153/153 [==============================] - 51s 336ms/step - loss: 0.0197 - lr: 0.0010\n",
      "Epoch 92/500\n",
      "153/153 [==============================] - 52s 342ms/step - loss: 0.0143 - lr: 0.0010\n",
      "Epoch 93/500\n",
      "153/153 [==============================] - 51s 336ms/step - loss: 0.0183 - lr: 0.0010\n",
      "Epoch 94/500\n",
      "153/153 [==============================] - 52s 337ms/step - loss: 0.0144 - lr: 0.0010\n",
      "Epoch 95/500\n",
      "153/153 [==============================] - 53s 343ms/step - loss: 0.0110 - lr: 0.0010\n",
      "Epoch 96/500\n",
      "153/153 [==============================] - 51s 336ms/step - loss: 0.0139 - lr: 0.0010\n",
      "Epoch 97/500\n",
      "153/153 [==============================] - 52s 342ms/step - loss: 0.0096 - lr: 0.0010\n",
      "Epoch 98/500\n",
      "153/153 [==============================] - 52s 337ms/step - loss: 0.0103 - lr: 0.0010\n",
      "Epoch 99/500\n",
      "153/153 [==============================] - 52s 342ms/step - loss: 0.0090 - lr: 0.0010\n",
      "Epoch 100/500\n",
      "153/153 [==============================] - 52s 341ms/step - loss: 0.0088 - lr: 0.0010\n",
      "Epoch 101/500\n",
      "153/153 [==============================] - 51s 336ms/step - loss: 0.0097 - lr: 4.0000e-04\n",
      "Epoch 102/500\n",
      "153/153 [==============================] - 51s 336ms/step - loss: 0.0088 - lr: 4.0000e-04\n",
      "Epoch 103/500\n",
      "153/153 [==============================] - 51s 335ms/step - loss: 0.0089 - lr: 4.0000e-04\n",
      "Epoch 104/500\n",
      "153/153 [==============================] - 52s 340ms/step - loss: 0.0046 - lr: 1.6000e-04\n",
      "Epoch 105/500\n",
      "153/153 [==============================] - 51s 335ms/step - loss: 0.0065 - lr: 1.6000e-04\n",
      "Epoch 106/500\n",
      "153/153 [==============================] - 52s 338ms/step - loss: 0.0086 - lr: 1.6000e-04\n",
      "Epoch 107/500\n",
      "153/153 [==============================] - 51s 336ms/step - loss: 0.0052 - lr: 1.6000e-04\n",
      "Epoch 108/500\n",
      "153/153 [==============================] - 52s 341ms/step - loss: 0.0033 - lr: 6.4000e-05\n",
      "Epoch 109/500\n",
      "153/153 [==============================] - 51s 335ms/step - loss: 0.0074 - lr: 6.4000e-05\n",
      "Epoch 110/500\n",
      "153/153 [==============================] - 52s 337ms/step - loss: 0.0037 - lr: 6.4000e-05\n",
      "Epoch 111/500\n",
      "153/153 [==============================] - 52s 337ms/step - loss: 0.0049 - lr: 6.4000e-05\n",
      "Epoch 112/500\n",
      "153/153 [==============================] - 52s 337ms/step - loss: 0.0037 - lr: 2.5600e-05\n",
      "Epoch 113/500\n",
      "153/153 [==============================] - 51s 335ms/step - loss: 0.0069 - lr: 2.5600e-05\n",
      "Epoch 114/500\n",
      "153/153 [==============================] - 52s 342ms/step - loss: 0.0027 - lr: 2.5600e-05\n",
      "Epoch 115/500\n",
      "153/153 [==============================] - 51s 336ms/step - loss: 0.0047 - lr: 1.0240e-05\n",
      "Epoch 116/500\n",
      "153/153 [==============================] - 51s 336ms/step - loss: 0.0030 - lr: 1.0240e-05\n",
      "Epoch 117/500\n",
      "153/153 [==============================] - 51s 336ms/step - loss: 0.0036 - lr: 1.0240e-05\n",
      "Epoch 118/500\n",
      "153/153 [==============================] - 52s 342ms/step - loss: 0.0018 - lr: 4.0960e-06\n",
      "Epoch 119/500\n",
      "153/153 [==============================] - 51s 335ms/step - loss: 0.0018 - lr: 4.0960e-06\n",
      "Epoch 120/500\n",
      "153/153 [==============================] - 51s 335ms/step - loss: 0.0025 - lr: 4.0960e-06\n",
      "Epoch 121/500\n",
      "153/153 [==============================] - 51s 335ms/step - loss: 0.0046 - lr: 4.0960e-06\n",
      "Epoch 122/500\n",
      "153/153 [==============================] - 52s 341ms/step - loss: 0.0016 - lr: 1.6384e-06\n",
      "Epoch 123/500\n",
      "153/153 [==============================] - 52s 340ms/step - loss: 9.4251e-04 - lr: 1.6384e-06\n",
      "Epoch 124/500\n",
      "153/153 [==============================] - 51s 336ms/step - loss: 0.0015 - lr: 1.6384e-06\n",
      "Epoch 125/500\n",
      "153/153 [==============================] - 51s 335ms/step - loss: 0.0015 - lr: 6.5536e-07\n",
      "Epoch 126/500\n",
      "153/153 [==============================] - 51s 335ms/step - loss: 0.0023 - lr: 6.5536e-07\n",
      "Epoch 127/500\n",
      "153/153 [==============================] - 51s 335ms/step - loss: 0.0015 - lr: 6.5536e-07\n",
      "Epoch 128/500\n",
      "153/153 [==============================] - 52s 341ms/step - loss: 3.2472e-04 - lr: 2.6214e-07\n",
      "Epoch 129/500\n",
      "153/153 [==============================] - 52s 337ms/step - loss: 0.0011 - lr: 2.6214e-07\n",
      "Epoch 130/500\n",
      "153/153 [==============================] - 51s 336ms/step - loss: 0.0016 - lr: 2.6214e-07\n",
      "Epoch 131/500\n",
      "153/153 [==============================] - 51s 337ms/step - loss: 0.0019 - lr: 2.6214e-07\n",
      "Epoch 132/500\n",
      "153/153 [==============================] - 51s 336ms/step - loss: 9.0255e-04 - lr: 1.0486e-07\n",
      "Epoch 133/500\n",
      "153/153 [==============================] - 51s 336ms/step - loss: 8.9891e-04 - lr: 1.0486e-07\n",
      "Epoch 134/500\n",
      "153/153 [==============================] - 51s 336ms/step - loss: 9.1227e-04 - lr: 1.0486e-07\n",
      "Epoch 135/500\n",
      "153/153 [==============================] - 51s 336ms/step - loss: 8.9237e-04 - lr: 4.1943e-08\n",
      "Epoch 136/500\n",
      "153/153 [==============================] - 52s 341ms/step - loss: 3.2148e-04 - lr: 4.1943e-08\n",
      "Epoch 137/500\n",
      "153/153 [==============================] - 51s 335ms/step - loss: 3.2263e-04 - lr: 4.1943e-08\n",
      "Epoch 138/500\n",
      "153/153 [==============================] - 51s 336ms/step - loss: 8.9523e-04 - lr: 1.6777e-08\n",
      "Epoch 139/500\n",
      "153/153 [==============================] - 51s 335ms/step - loss: 3.2259e-04 - lr: 1.6777e-08\n",
      "Epoch 140/500\n",
      "153/153 [==============================] - 51s 335ms/step - loss: 3.2231e-04 - lr: 1.6777e-08\n",
      "Epoch 141/500\n",
      "153/153 [==============================] - 51s 336ms/step - loss: 3.2178e-04 - lr: 6.7109e-09\n",
      "Epoch 142/500\n",
      "153/153 [==============================] - 51s 336ms/step - loss: 3.2162e-04 - lr: 6.7109e-09\n",
      "Epoch 143/500\n",
      "153/153 [==============================] - 52s 341ms/step - loss: 3.2132e-04 - lr: 6.7109e-09\n",
      "Epoch 144/500\n",
      "153/153 [==============================] - 51s 336ms/step - loss: 3.2133e-04 - lr: 2.6844e-09\n",
      "Epoch 145/500\n",
      "153/153 [==============================] - 51s 336ms/step - loss: 8.9628e-04 - lr: 2.6844e-09\n",
      "Epoch 146/500\n",
      "153/153 [==============================] - 52s 337ms/step - loss: 3.2229e-04 - lr: 2.6844e-09\n",
      "Epoch 147/500\n",
      "153/153 [==============================] - 51s 336ms/step - loss: 3.2157e-04 - lr: 1.0737e-09\n",
      "Epoch 148/500\n",
      "153/153 [==============================] - 52s 342ms/step - loss: 3.2056e-04 - lr: 1.0737e-09\n",
      "Epoch 149/500\n",
      "153/153 [==============================] - 51s 336ms/step - loss: 3.2166e-04 - lr: 1.0737e-09\n",
      "Epoch 150/500\n",
      "153/153 [==============================] - 52s 337ms/step - loss: 8.6902e-04 - lr: 4.2950e-10\n",
      "Epoch 151/500\n",
      "153/153 [==============================] - 52s 338ms/step - loss: 3.2147e-04 - lr: 4.2950e-10\n",
      "Epoch 152/500\n",
      "153/153 [==============================] - 52s 337ms/step - loss: 0.0013 - lr: 4.2950e-10\n",
      "Epoch 153/500\n",
      "153/153 [==============================] - 52s 337ms/step - loss: 3.2086e-04 - lr: 1.7180e-10\n",
      "Epoch 154/500\n",
      "153/153 [==============================] - 51s 336ms/step - loss: 0.0012 - lr: 1.7180e-10\n",
      "Epoch 155/500\n",
      "153/153 [==============================] - 51s 336ms/step - loss: 9.0122e-04 - lr: 1.7180e-10\n",
      "Epoch 156/500\n",
      "153/153 [==============================] - 52s 337ms/step - loss: 3.2167e-04 - lr: 6.8719e-11\n",
      "Epoch 157/500\n",
      "153/153 [==============================] - 51s 337ms/step - loss: 8.9459e-04 - lr: 6.8719e-11\n",
      "Epoch 158/500\n",
      "153/153 [==============================] - 51s 336ms/step - loss: 3.2282e-04 - lr: 6.8719e-11\n",
      "Epoch 159/500\n",
      "153/153 [==============================] - 52s 338ms/step - loss: 9.1775e-04 - lr: 2.7488e-11\n",
      "Epoch 160/500\n",
      "153/153 [==============================] - 51s 336ms/step - loss: 3.2214e-04 - lr: 2.7488e-11\n",
      "Epoch 161/500\n",
      "153/153 [==============================] - 51s 336ms/step - loss: 3.2154e-04 - lr: 2.7488e-11\n",
      "Epoch 162/500\n",
      "153/153 [==============================] - 51s 336ms/step - loss: 0.0015 - lr: 1.0995e-11\n",
      "Epoch 163/500\n",
      "153/153 [==============================] - 52s 337ms/step - loss: 3.2207e-04 - lr: 1.0995e-11\n"
     ]
    }
   ],
   "source": [
    "model = create_classifier(learning_rate)\n",
    "        \n",
    "lr = tf.keras.callbacks.ReduceLROnPlateau(monitor = 'loss', factor = 0.4, patience = 3, verbose = 0, min_delta = 0.001, mode = 'min')\n",
    "es = tf.keras.callbacks.EarlyStopping(monitor='loss', mode='min', verbose=0, patience=15, restore_best_weights=True)\n",
    "mc = tf.keras.callbacks.ModelCheckpoint(ckp_path, monitor='loss', mode='min', verbose=0, save_best_only=True, save_weights_only=True)\n",
    "        \n",
    "# train the model\n",
    "history = model.fit(x=train_generator,\n",
    "                    steps_per_epoch=STEP_SIZE_TRAIN,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=num_epochs,\n",
    "                    callbacks=[mc,lr,es],  \n",
    "                    shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e9d20534",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_classifier(learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ad662b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(ckp_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "035967dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_layer = model.get_layer('embeddings')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1d732674",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = embedding_layer.get_weights()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ceb69ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(\"data/embeddings_model_w2v_v3.csv\", embeddings, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "311cb1b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_index = tokenizer.word_index\n",
    "index_to_word = dict()\n",
    "\n",
    "for key in word_to_index:\n",
    "    index_to_word.update({word_to_index[key] : key })\n",
    "\n",
    "word_to_index.update({'unk':0})\n",
    "index_to_word.update({0:'unk'})  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "28d31a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('data/word_to_index_v3.pkl', 'wb') as fp:\n",
    "    pickle.dump(word_to_index, fp)\n",
    "    \n",
    "with open('data/index_to_word_v3.pkl', 'wb') as fp:\n",
    "    pickle.dump(index_to_word, fp)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d2fdd0f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#embeddings[tokenizer.word_index['unk']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "41f1819d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.13304226"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity(embeddings[word_to_index['il']].reshape(1,300),embeddings[word_to_index['or']].reshape(1,300))[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "98fbd0b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.013190172"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity(embeddings[word_to_index['60126']].reshape(1,300),embeddings[word_to_index['60181']].reshape(1,300))[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "fb52445a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.06738729"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity(embeddings[word_to_index['60126']].reshape(1,300),embeddings[word_to_index['97035']].reshape(1,300))[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d312d871",
   "metadata": {},
   "outputs": [],
   "source": [
    "il_60126 = np.reshape(embeddings[word_to_index['il']],embeddings_shape) + np.reshape(embeddings[word_to_index['60126']],embeddings_shape)\n",
    "il_60181 = np.reshape(embeddings[word_to_index['il']],embeddings_shape) + np.reshape(embeddings[word_to_index['60181']],embeddings_shape)\n",
    "or_97035 = np.reshape(embeddings[word_to_index['or']],embeddings_shape) + np.reshape(embeddings[word_to_index['97035']],embeddings_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "87bad735",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6812738"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity(il_60126,il_60181)[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "747cca0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.038763817"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity(il_60126,or_97035)[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4e4e2563",
   "metadata": {},
   "outputs": [],
   "source": [
    "il_60126 += np.reshape(embeddings[word_to_index['dupage'],:],embeddings_shape)\n",
    "il_60181 += np.reshape(embeddings[word_to_index['dupage'],:],embeddings_shape)\n",
    "or_97035 += np.reshape(embeddings[word_to_index['clackamas'],:],embeddings_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9037236c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.81761783"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity(il_60126,il_60181)[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "dd514a5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.26062876"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity(il_60126,or_97035)[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76377658",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05027c6c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
